{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Whisper model\n",
    "whis_model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer for retrieval\n",
    "retrive_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy RAG document store\n",
    "doc = {\n",
    "    \"doc1\": \"RAG is a generative model that can be used for variety of tasks including speech recognition , translation and summarization.\",\n",
    "    \"doc2\": \"Building a multilingual speech recognition without training. \",\n",
    "    \"doc3\": \"Here we discuss that the field of multilingual AI is rapidly evolving\",\n",
    "    \"doc4\": \"Multilingual embedding models are essential for RAG systems, enabling robust cross-lingual information retrieval and generation.\",\n",
    "    \"doc5\": [\n",
    "        \"Key considerations for choosing a multilingual embedding model include language coverage, dimensionality, and integration ease\",\n",
    "        \"In the era of global communication, developing effective multilingual AI systems has become increasingly important.\",\n",
    "        \"Multilingual-RAG is built upon the powerful architecture of Large Language Models (LLMs) with Retrieve-And-Generate (RAG) capabilities\",\n",
    "        \"multilingual speech recognition  means you can now speak naturally in your preferred language and have your device or computer understand you perfectly, no matter what language you're speaking.\",\n",
    "        \"This project aims to make it easier for people to access information and use devices in their preferred language\",\n",
    "        \"Education will benefit from personalized learning experiences tailored to individual student needs.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten the document content for encoding\n",
    "doc_text = []\n",
    "for key, value in doc.items():\n",
    "    if isinstance(value, list):\n",
    "        doc_text.extend(value)\n",
    "    else:\n",
    "        doc_text.append(value)\n",
    "\n",
    "# Encode the documents using the retriever model\n",
    "doc_embedding = retrive_model.encode(doc_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe speech using Whisper\n",
    "def transcribe_audio(audio_path):\n",
    "    result = whis_model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Function to detect language\n",
    "def d_language(audio_path):\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(whis_model.device)\n",
    "    _, probs = whis_model.detect_language(mel)\n",
    "    detect_lang = max(probs, key=probs.get)\n",
    "\n",
    "    # Map detected language codes to readable names\n",
    "    language_map = {\n",
    "        'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',\n",
    "        'hi': 'Hindi', 'ja': 'Japanese', 'ru': 'Russian', 'ar': 'Arabic',\n",
    "        'te': 'Telugu', 'zh': 'Chinese', 'pt': 'Portuguese'\n",
    "    }\n",
    "\n",
    "    return language_map.get(detect_lang, detect_lang).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load translation model and tokenizer\n",
    "def load_translation(source_language, target_language):\n",
    "    model_name = {\n",
    "        \"English-Hindi\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
    "        \"English-Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
    "        \"English-Japanese\": \"Helsinki-NLP/opus-mt-en-jap\",\n",
    "        \"English-German\": \"Helsinki-NLP/opus-mt-en-de\",\n",
    "        \"English-Russian\": \"Helsinki-NLP/opus-mt-en-ru\",\n",
    "        \"English-Arabic\": \"Helsinki-NLP/opus-mt-en-ar\",\n",
    "        \"English-Telugu\": \"Helsinki-NLP/opus-mt-en-te\",\n",
    "        \"English-French\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "        \"English-Italian\": \"Helsinki-NLP/opus-mt-en-it\",\n",
    "        \"Hindi-English\": \"Helsinki-NLP/opus-mt-hi-en\",\n",
    "        \"Spanish-English\": \"Helsinki-NLP/opus-mt-es-en\",\n",
    "        \"Japanese-English\": \"Helsinki-NLP/opus-mt-jap-en\",\n",
    "        \"German-English\": \"Helsinki-NLP/opus-mt-de-en\",\n",
    "        \"Russian-English\": \"Helsinki-NLP/opus-mt-ru-en\",\n",
    "        \"Arabic-English\": \"Helsinki-NLP/opus-mt-ar-en\",\n",
    "        \"Telugu-English\": \"Helsinki-NLP/opus-mt-te-en\",\n",
    "        \"French-English\": \"Helsinki-NLP/opus-mt-fr-en\",\n",
    "        \"Italian-English\": \"Helsinki-NLP/opus-mt-it-en\",\n",
    "    }\n",
    "    key = f\"{source_language}-{target_language}\"\n",
    "    if key not in model_name:\n",
    "        raise ValueError(f\"Translation model for {source_language} to {target_language} not available.\")\n",
    "\n",
    "    translate_model = MarianMTModel.from_pretrained(model_name[key])\n",
    "    translate_tokenizer = MarianTokenizer.from_pretrained(model_name[key])\n",
    "    return translate_model, translate_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate text\n",
    "def translate_t(text, model, tokenizer):\n",
    "    input = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        translated_token = model.generate(**input)\n",
    "    translation = tokenizer.decode(translated_token[0], skip_special_tokens=True)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_doc(query):\n",
    "    q_embedding = retrive_model.encode(query, convert_to_tensor=True)\n",
    "    score = util.pytorch_cos_sim(q_embedding, doc_embedding)[0]\n",
    "\n",
    "    # Ensure there are valid scores\n",
    "    if len(score) == 0 or torch.isnan(score).any():\n",
    "        raise ValueError(\"No valid scores found for the query.\")\n",
    "\n",
    "    top_score_idx = score.argmax().item()\n",
    "\n",
    "    # Validate the index\n",
    "    if 0 <= top_score_idx < len(doc_text):\n",
    "        return doc_text[top_score_idx]\n",
    "    else:\n",
    "        raise IndexError(\"Top score index out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suchi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "# Function to process the audio file and return transcriptions and translations\n",
    "def process_audio(audio, target_language):\n",
    "    # Transcribe the audio\n",
    "    transcription = transcribe_audio(audio)\n",
    "\n",
    "    # Detect the language spoken in the audio\n",
    "    detected_language = d_language(audio)\n",
    "\n",
    "    # Translate the transcribed text to English if it's not in English\n",
    "    if detected_language != \"English\":\n",
    "        translation_model, translation_tokenizer = load_translation(detected_language, \"English\")\n",
    "        transcription = translate_t(transcription, translation_model, translation_tokenizer)\n",
    "        \n",
    "    # Load the appropriate translation model to the target language\n",
    "    translation_model, translation_tokenizer = load_translation(\"English\", target_language)\n",
    "    \n",
    "    # Translate the transcribed text to the target language\n",
    "    translated_text = translate_t(transcription, translation_model, translation_tokenizer)\n",
    "    \n",
    "    # Retrieve document based on the transcribed text\n",
    "    retrieved_document = retrieve_doc(transcription)\n",
    "\n",
    "    return transcription, detected_language, translated_text, retrieved_document\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=[\n",
    "        gr.Audio(type=\"filepath\"),\n",
    "        gr.Dropdown([\"Hindi\", \"Spanish\", \"Japanese\", \"German\", \"Russian\", \"Arabic\", \"French\", \"Italian\", \"English\"], label=\"Target Language\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Transcription\"),\n",
    "        gr.Textbox(label=\"Detected Language\"),\n",
    "        gr.Textbox(label=\"Translation\"),\n",
    "        gr.Textbox(label=\"Retrieved Document\")\n",
    "    ],\n",
    "    title=\"Multilingual Speech Recognition, Translation, and Document Retrieval\",\n",
    "    description=\"Upload an audio file in any language, select a target language to get the transcription, translation, and retrieve a document based on the transcription.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
